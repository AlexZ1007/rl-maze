{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b014208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Simona Istoc\\Desktop\\rl-maze\\rl-env\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import minigrid\n",
    "from gymnasium.envs.registration import register\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4effa8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 0) Register your custom env (assumes enemy_doorkey_env.py exists)\n",
    "# ==========================================================\n",
    "register(\n",
    "    id=\"MiniGrid-DoorKey-6x6-Enemy-v0\",\n",
    "    entry_point=\"enemy_doorkey_env:DoorKeyWithEnemyEnv\",\n",
    "    kwargs={\"size\":6}\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# 1) Useful actions only (DoorKey)\n",
    "# ==========================================================\n",
    "USEFUL_ACTIONS = [0, 1, 2, 3, 5]  # left, right, forward, pickup, toggle\n",
    "\n",
    "def sample_useful_action():\n",
    "    return int(np.random.choice(USEFUL_ACTIONS))\n",
    "\n",
    "# ==========================================================\n",
    "# 2) State encoder (no enemy_pos -> smaller table)\n",
    "# ==========================================================\n",
    "def get_door_open(u):\n",
    "    for j in range(u.height):\n",
    "        for i in range(u.width):\n",
    "            obj = u.grid.get(i, j)\n",
    "            if obj is not None and obj.type == \"door\":\n",
    "                return 1 if obj.is_open else 0\n",
    "    return 0\n",
    "\n",
    "def get_state(env):\n",
    "    u = env.unwrapped\n",
    "    ax, ay = u.agent_pos\n",
    "    ad = int(u.agent_dir)\n",
    "    has_key = 1 if (u.carrying is not None and getattr(u.carrying, \"type\", None) == \"key\") else 0\n",
    "    door_open = get_door_open(u)\n",
    "    # Return as a tuple, which we will convert to tensor later\n",
    "    return np.array([ax, ay, ad, has_key, door_open], dtype=np.float32)\n",
    "\n",
    "# ==========================================================\n",
    "# 3) Distance-to-goal shaping helpers (aligned with success)\n",
    "# ==========================================================\n",
    "def find_goal_pos(u):\n",
    "    for j in range(u.height):\n",
    "        for i in range(u.width):\n",
    "            obj = u.grid.get(i, j)\n",
    "            if obj is not None and obj.type == \"goal\":\n",
    "                return (i, j)\n",
    "    return None\n",
    "\n",
    "def manhattan(a, b):\n",
    "    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "# ==========================================================\n",
    "# 4) PPO Actor-Critic Model and Training Loop\n",
    "# ==========================================================\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, hidden=64):\n",
    "        super().__init__()\n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, hidden)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden, hidden)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Actor head (Policy)\n",
    "        self.actor = layer_init(nn.Linear(hidden, n_actions), std=0.01)\n",
    "        \n",
    "        # Critic head (Value)\n",
    "        self.critic = layer_init(nn.Linear(hidden, 1), std=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        logits = self.actor(h)\n",
    "        value = self.critic(h)\n",
    "        return logits, value\n",
    "\n",
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE).\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    advs = []\n",
    "    gae = 0\n",
    "    next_value = 0\n",
    "    \n",
    "    # Iterate backwards\n",
    "    for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):\n",
    "        delta = r + gamma * next_value * (1 - d) - v\n",
    "        gae = delta + gamma * lam * (1 - d) * gae\n",
    "        advs.insert(0, gae)\n",
    "        next_value = v\n",
    "        returns.insert(0, gae + v)\n",
    "        \n",
    "    return torch.tensor(returns, dtype=torch.float32), torch.tensor(advs, dtype=torch.float32)\n",
    "\n",
    "def train_ppo(\n",
    "    env,\n",
    "    model,\n",
    "    optimizer,\n",
    "    episodes=2000,\n",
    "    max_steps=500,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    eps_clip=0.2,\n",
    "    update_epochs=4,\n",
    "    batch_size=64,\n",
    "    dist_coef=0.02,\n",
    "    living_penalty=-0.002,\n",
    "    entropy_coef=0.01,\n",
    "):\n",
    "    rewards_history, success_history = [], []\n",
    "    \n",
    "    # Logging accumulators for the print block\n",
    "    actor_loss_sum = 0.0\n",
    "    critic_loss_sum = 0.0\n",
    "    val_abs_sum = 0.0\n",
    "    val_max = 0.0\n",
    "    updates_count = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        s = get_state(env)\n",
    "        goal = find_goal_pos(env.unwrapped)\n",
    "        if goal is None: raise RuntimeError(\"Goal not found.\")\n",
    "        \n",
    "        log_probs_buf, values_buf, states_buf, actions_buf, rewards_buf, dones_buf = [], [], [], [], [], []\n",
    "        total_shaped_reward = 0.0\n",
    "        last_env_r = 0.0\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            s_tensor = torch.tensor(s).unsqueeze(0)\n",
    "            logits, value = model(s_tensor)\n",
    "            m = Categorical(logits=logits)\n",
    "            action_idx = m.sample()\n",
    "            action = USEFUL_ACTIONS[action_idx.item()]\n",
    "            \n",
    "            prev_pos = env.unwrapped.agent_pos\n",
    "            obs2, r, terminated, truncated, info = env.step(action)\n",
    "            new_pos = env.unwrapped.agent_pos\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            shaped_r = max(float(r), -0.2) + dist_coef * (manhattan(prev_pos, goal) - manhattan(new_pos, goal)) + living_penalty\n",
    "            \n",
    "            log_probs_buf.append(m.log_prob(action_idx))\n",
    "            values_buf.append(value)\n",
    "            states_buf.append(s_tensor)\n",
    "            actions_buf.append(action_idx)\n",
    "            rewards_buf.append(shaped_r)\n",
    "            dones_buf.append(done)\n",
    "            \n",
    "            total_shaped_reward += shaped_r\n",
    "            last_env_r = float(r)\n",
    "            s = get_state(env)\n",
    "            \n",
    "            # Log value stats for this step\n",
    "            v_scalar = value.item()\n",
    "            val_abs_sum += abs(v_scalar)\n",
    "            val_max = max(val_max, abs(v_scalar))\n",
    "            updates_count += 1\n",
    "            \n",
    "            if done: break\n",
    "\n",
    "        rewards_history.append(total_shaped_reward)\n",
    "        success_history.append(1 if last_env_r > 0 else 0)\n",
    "        \n",
    "        values_tensor = torch.stack(values_buf).reshape(-1)\n",
    "        log_probs_tensor = torch.stack(log_probs_buf).reshape(-1)\n",
    "        states_tensor = torch.cat(states_buf)\n",
    "        actions_tensor = torch.stack(actions_buf).reshape(-1)\n",
    "        \n",
    "        returns, advantages = compute_gae(rewards_buf, values_tensor.detach().numpy(), dones_buf, gamma, lam)\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        dataset_size = len(states_tensor)\n",
    "        indices = np.arange(dataset_size)\n",
    "        \n",
    "        for _ in range(update_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, dataset_size, batch_size):\n",
    "                end = start + batch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                mb_states = states_tensor[idx]\n",
    "                mb_actions = actions_tensor[idx]\n",
    "                mb_old_log_probs = log_probs_tensor[idx].detach()\n",
    "                mb_returns = returns[idx]\n",
    "                mb_advantages = advantages[idx]\n",
    "                \n",
    "                new_logits, new_values = model(mb_states)\n",
    "                new_dist = Categorical(logits=new_logits)\n",
    "                new_log_probs = new_dist.log_prob(mb_actions)\n",
    "                entropy = new_dist.entropy().mean()\n",
    "                \n",
    "                ratio = (new_log_probs - mb_old_log_probs).exp()\n",
    "                surr1 = ratio * mb_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * mb_advantages\n",
    "                \n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = 0.5 * ((mb_returns - new_values.reshape(-1)) ** 2).mean()\n",
    "                loss = actor_loss + critic_loss - entropy_coef * entropy\n",
    "                \n",
    "                # Accumulate loss stats\n",
    "                actor_loss_sum += actor_loss.item()\n",
    "                critic_loss_sum += critic_loss.item()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "        if (ep + 1) % 500 == 0:\n",
    "            avg_rew = np.mean(rewards_history[-500:])\n",
    "            succ_rate = np.mean(success_history[-500:])\n",
    "            \n",
    "            # Normalize stats by number of updates/steps\n",
    "            # (Note: loss sums are accumulated over many mini-batches, so we divide to get avg)\n",
    "            norm_factor = max(1, updates_count * update_epochs / batch_size) \n",
    "            avg_actor_loss = actor_loss_sum / norm_factor\n",
    "            avg_critic_loss = critic_loss_sum / norm_factor\n",
    "            \n",
    "            avg_val_abs = val_abs_sum / max(1, updates_count)\n",
    "            \n",
    "            print(\n",
    "                f\"Episode {ep+1}/{episodes} | \"\n",
    "                f\"avg_reward(last500)={avg_rew:.3f} | \"\n",
    "                f\"success_rate(last500)={succ_rate:.2%} | \"\n",
    "                f\"ActorLoss={avg_actor_loss:.4f} | \"\n",
    "                f\"CriticLoss={avg_critic_loss:.4f} | \"\n",
    "                f\"Val(avg/max)={avg_val_abs:.3f}/{val_max:.3f}\"\n",
    "            )\n",
    "            \n",
    "            # Reset trackers for next 500 episodes\n",
    "            actor_loss_sum = 0.0\n",
    "            critic_loss_sum = 0.0\n",
    "            val_abs_sum = 0.0\n",
    "            val_max = 0.0\n",
    "            updates_count = 0\n",
    "\n",
    "    return rewards_history, success_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475d2db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Phase 1: train on MiniGrid-DoorKey-6x6-v0 ===\n",
      "Episode 500/3000 | avg_reward(last500)=-0.032 | success_rate(last500)=68.40% | ActorLoss=-0.0061 | CriticLoss=0.0034 | Val(avg/max)=0.174/0.940\n",
      "Episode 1000/3000 | avg_reward(last500)=0.188 | success_rate(last500)=78.60% | ActorLoss=-0.0060 | CriticLoss=0.0053 | Val(avg/max)=0.225/0.966\n",
      "Episode 1500/3000 | avg_reward(last500)=0.314 | success_rate(last500)=86.60% | ActorLoss=-0.0063 | CriticLoss=0.0065 | Val(avg/max)=0.274/0.947\n",
      "Episode 2000/3000 | avg_reward(last500)=0.351 | success_rate(last500)=85.80% | ActorLoss=-0.0067 | CriticLoss=0.0079 | Val(avg/max)=0.298/0.981\n",
      "Episode 2500/3000 | avg_reward(last500)=0.543 | success_rate(last500)=96.60% | ActorLoss=-0.0117 | CriticLoss=0.0097 | Val(avg/max)=0.376/1.121\n",
      "Episode 3000/3000 | avg_reward(last500)=0.500 | success_rate(last500)=93.40% | ActorLoss=-0.0042 | CriticLoss=0.0093 | Val(avg/max)=0.372/1.080\n",
      "\n",
      "=== Phase 2: continue on MiniGrid-DoorKey-6x6-Enemy-v0 ===\n",
      "Episode 500/8000 | avg_reward(last500)=0.056 | success_rate(last500)=34.20% | ActorLoss=-0.0061 | CriticLoss=0.0348 | Val(avg/max)=0.111/0.738\n",
      "Episode 1000/8000 | avg_reward(last500)=-0.080 | success_rate(last500)=22.60% | ActorLoss=-0.0032 | CriticLoss=0.0217 | Val(avg/max)=0.103/0.650\n",
      "Episode 1500/8000 | avg_reward(last500)=-0.062 | success_rate(last500)=24.80% | ActorLoss=-0.0020 | CriticLoss=0.0213 | Val(avg/max)=0.108/0.677\n",
      "Episode 2000/8000 | avg_reward(last500)=-0.046 | success_rate(last500)=26.40% | ActorLoss=-0.0042 | CriticLoss=0.0233 | Val(avg/max)=0.107/0.784\n",
      "Episode 2500/8000 | avg_reward(last500)=-0.079 | success_rate(last500)=26.40% | ActorLoss=0.0001 | CriticLoss=0.0174 | Val(avg/max)=0.099/0.700\n",
      "Episode 3000/8000 | avg_reward(last500)=-0.088 | success_rate(last500)=22.40% | ActorLoss=0.0042 | CriticLoss=0.0190 | Val(avg/max)=0.101/0.647\n",
      "Episode 3500/8000 | avg_reward(last500)=-0.082 | success_rate(last500)=24.80% | ActorLoss=0.0042 | CriticLoss=0.0198 | Val(avg/max)=0.098/0.763\n",
      "Episode 4000/8000 | avg_reward(last500)=-0.065 | success_rate(last500)=26.40% | ActorLoss=0.0016 | CriticLoss=0.0197 | Val(avg/max)=0.100/0.712\n",
      "Episode 4500/8000 | avg_reward(last500)=-0.167 | success_rate(last500)=20.40% | ActorLoss=0.0000 | CriticLoss=0.0108 | Val(avg/max)=0.120/0.586\n",
      "Episode 5000/8000 | avg_reward(last500)=-0.225 | success_rate(last500)=17.00% | ActorLoss=-0.0033 | CriticLoss=0.0065 | Val(avg/max)=0.107/0.537\n",
      "Episode 5500/8000 | avg_reward(last500)=-0.215 | success_rate(last500)=17.00% | ActorLoss=-0.0049 | CriticLoss=0.0075 | Val(avg/max)=0.101/0.664\n",
      "Episode 6000/8000 | avg_reward(last500)=-0.215 | success_rate(last500)=18.40% | ActorLoss=-0.0049 | CriticLoss=0.0072 | Val(avg/max)=0.110/0.614\n",
      "Episode 6500/8000 | avg_reward(last500)=-0.161 | success_rate(last500)=20.60% | ActorLoss=-0.0039 | CriticLoss=0.0108 | Val(avg/max)=0.091/0.671\n",
      "Episode 7000/8000 | avg_reward(last500)=-0.137 | success_rate(last500)=21.80% | ActorLoss=-0.0004 | CriticLoss=0.0117 | Val(avg/max)=0.089/0.709\n",
      "Episode 7500/8000 | avg_reward(last500)=-0.142 | success_rate(last500)=21.80% | ActorLoss=0.0012 | CriticLoss=0.0117 | Val(avg/max)=0.095/0.770\n",
      "Episode 8000/8000 | avg_reward(last500)=-0.140 | success_rate(last500)=22.80% | ActorLoss=-0.0018 | CriticLoss=0.0115 | Val(avg/max)=0.096/0.755\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "env_easy = gym.make(\"MiniGrid-DoorKey-6x6-v0\")\n",
    "env_hard = gym.make(\"MiniGrid-DoorKey-6x6-Enemy-v0\")\n",
    "state_dim = 5\n",
    "n_actions = len(USEFUL_ACTIONS)\n",
    "\n",
    "model = ActorCritic(state_dim, n_actions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, eps=1e-5)\n",
    "\n",
    "print(\"\\n=== Phase 1: train on MiniGrid-DoorKey-6x6-v0 ===\")\n",
    "rewards1, success1 = train_ppo(\n",
    "    env_easy, \n",
    "    model, \n",
    "    optimizer, \n",
    "    episodes=3000, \n",
    "    dist_coef=0.01,       # LOWER: Less hand-holding\n",
    "    entropy_coef=0.005,   # LOWER: Exploit faster\n",
    "    eps_clip=0.3,         # HIGHER: Allow bigger updates\n",
    "    update_epochs=5       # HIGHER: Train more on each batch\n",
    ")\n",
    "\n",
    "print(\"\\n=== Phase 2: continue on MiniGrid-DoorKey-6x6-Enemy-v0 ===\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.5e-4, eps=1e-5)\n",
    "\n",
    "rewards2, success2 = train_ppo(\n",
    "    env_hard, \n",
    "    model, \n",
    "    optimizer, \n",
    "    episodes=8000, \n",
    "    dist_coef=0.01,       # Keep sparse shaping\n",
    "    entropy_coef=0.02,    # Standard entropy for hard phase\n",
    "    update_epochs=4,      # Standard epochs\n",
    "    eps_clip=0.2          # Standard clip\n",
    ")\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
