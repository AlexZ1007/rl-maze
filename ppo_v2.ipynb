{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b014208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import minigrid\n",
    "from gymnasium.envs.registration import register\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4effa8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Simona Istoc\\Desktop\\rl-maze\\rl-env\\Lib\\site-packages\\gymnasium\\envs\\registration.py:636: UserWarning: \u001b[33mWARN: Overriding environment MiniGrid-DoorKey-6x6-Enemy-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 0) Register your custom env (assumes enemy_doorkey_env.py exists)\n",
    "# ==========================================================\n",
    "register(\n",
    "    id=\"MiniGrid-DoorKey-6x6-Enemy-v0\",\n",
    "    entry_point=\"enemy_doorkey_env:DoorKeyWithEnemyEnv\",\n",
    "    kwargs={\"size\":6}\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# 1) Useful actions only (DoorKey)\n",
    "# ==========================================================\n",
    "USEFUL_ACTIONS = [0, 1, 2, 3, 5]  # left, right, forward, pickup, toggle\n",
    "\n",
    "def sample_useful_action():\n",
    "    return int(np.random.choice(USEFUL_ACTIONS))\n",
    "\n",
    "# ==========================================================\n",
    "# 2) State encoder (no enemy_pos -> smaller table)\n",
    "# ==========================================================\n",
    "def get_door_open(u):\n",
    "    for j in range(u.height):\n",
    "        for i in range(u.width):\n",
    "            obj = u.grid.get(i, j)\n",
    "            if obj is not None and obj.type == \"door\":\n",
    "                return 1 if obj.is_open else 0\n",
    "    return 0\n",
    "\n",
    "def get_state(env):\n",
    "    u = env.unwrapped\n",
    "    ax, ay = u.agent_pos\n",
    "    ad = int(u.agent_dir)\n",
    "    has_key = 1 if (u.carrying is not None and getattr(u.carrying, \"type\", None) == \"key\") else 0\n",
    "    door_open = get_door_open(u)\n",
    "    # Return as a tuple, which we will convert to tensor later\n",
    "    return np.array([ax, ay, ad, has_key, door_open], dtype=np.float32)\n",
    "\n",
    "# ==========================================================\n",
    "# 3) Distance-to-goal shaping helpers (aligned with success)\n",
    "# ==========================================================\n",
    "def find_goal_pos(u):\n",
    "    for j in range(u.height):\n",
    "        for i in range(u.width):\n",
    "            obj = u.grid.get(i, j)\n",
    "            if obj is not None and obj.type == \"goal\":\n",
    "                return (i, j)\n",
    "    return None\n",
    "\n",
    "def manhattan(a, b):\n",
    "    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "# ==========================================================\n",
    "# 4) PPO Actor-Critic Model and Training Loop\n",
    "# ==========================================================\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, hidden=64):\n",
    "        super().__init__()\n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, hidden)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden, hidden)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Actor head (Policy)\n",
    "        self.actor = layer_init(nn.Linear(hidden, n_actions), std=0.01)\n",
    "        \n",
    "        # Critic head (Value)\n",
    "        self.critic = layer_init(nn.Linear(hidden, 1), std=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        logits = self.actor(h)\n",
    "        value = self.critic(h)\n",
    "        return logits, value\n",
    "\n",
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE).\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    advs = []\n",
    "    gae = 0\n",
    "    next_value = 0\n",
    "    \n",
    "    # Iterate backwards\n",
    "    for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):\n",
    "        delta = r + gamma * next_value * (1 - d) - v\n",
    "        gae = delta + gamma * lam * (1 - d) * gae\n",
    "        advs.insert(0, gae)\n",
    "        next_value = v\n",
    "        returns.insert(0, gae + v)\n",
    "        \n",
    "    return torch.tensor(returns, dtype=torch.float32), torch.tensor(advs, dtype=torch.float32)\n",
    "\n",
    "def train_ppo(\n",
    "    env,\n",
    "    model,\n",
    "    optimizer,\n",
    "    episodes=2000,\n",
    "    max_steps=500,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    eps_clip=0.2,\n",
    "    update_epochs=4,\n",
    "    batch_size=64,\n",
    "    dist_coef=0.02,\n",
    "    living_penalty=-0.001,\n",
    "    entropy_coef=0.01,\n",
    "):\n",
    "    rewards_history, success_history = [], []\n",
    "    \n",
    "    # Logging accumulators for the print block\n",
    "    actor_loss_sum = 0.0\n",
    "    critic_loss_sum = 0.0\n",
    "    val_abs_sum = 0.0\n",
    "    val_max = 0.0\n",
    "    updates_count = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        s = get_state(env)\n",
    "        goal = find_goal_pos(env.unwrapped)\n",
    "        if goal is None: raise RuntimeError(\"Goal not found.\")\n",
    "        \n",
    "        log_probs_buf, values_buf, states_buf, actions_buf, rewards_buf, dones_buf = [], [], [], [], [], []\n",
    "        total_shaped_reward = 0.0\n",
    "        last_env_r = 0.0\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            s_tensor = torch.tensor(s).unsqueeze(0)\n",
    "            logits, value = model(s_tensor)\n",
    "            m = Categorical(logits=logits)\n",
    "            action_idx = m.sample()\n",
    "            action = USEFUL_ACTIONS[action_idx.item()]\n",
    "            \n",
    "            prev_pos = env.unwrapped.agent_pos\n",
    "            obs2, r, terminated, truncated, info = env.step(action)\n",
    "            new_pos = env.unwrapped.agent_pos\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            shaped_r = max(float(r), -0.2) + dist_coef * (manhattan(prev_pos, goal) - manhattan(new_pos, goal)) + living_penalty\n",
    "            \n",
    "            log_probs_buf.append(m.log_prob(action_idx))\n",
    "            values_buf.append(value)\n",
    "            states_buf.append(s_tensor)\n",
    "            actions_buf.append(action_idx)\n",
    "            rewards_buf.append(shaped_r)\n",
    "            dones_buf.append(done)\n",
    "            \n",
    "            total_shaped_reward += shaped_r\n",
    "            last_env_r = float(r)\n",
    "            s = get_state(env)\n",
    "            \n",
    "            # Log value stats for this step\n",
    "            v_scalar = value.item()\n",
    "            val_abs_sum += abs(v_scalar)\n",
    "            val_max = max(val_max, abs(v_scalar))\n",
    "            updates_count += 1\n",
    "            \n",
    "            if done: break\n",
    "\n",
    "        rewards_history.append(total_shaped_reward)\n",
    "        success_history.append(1 if last_env_r > 0 else 0)\n",
    "        \n",
    "        values_tensor = torch.stack(values_buf).reshape(-1)\n",
    "        log_probs_tensor = torch.stack(log_probs_buf).reshape(-1)\n",
    "        states_tensor = torch.cat(states_buf)\n",
    "        actions_tensor = torch.stack(actions_buf).reshape(-1)\n",
    "        \n",
    "        returns, advantages = compute_gae(rewards_buf, values_tensor.detach().numpy(), dones_buf, gamma, lam)\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        dataset_size = len(states_tensor)\n",
    "        indices = np.arange(dataset_size)\n",
    "        \n",
    "        for _ in range(update_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, dataset_size, batch_size):\n",
    "                end = start + batch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                mb_states = states_tensor[idx]\n",
    "                mb_actions = actions_tensor[idx]\n",
    "                mb_old_log_probs = log_probs_tensor[idx].detach()\n",
    "                mb_returns = returns[idx]\n",
    "                mb_advantages = advantages[idx]\n",
    "                \n",
    "                new_logits, new_values = model(mb_states)\n",
    "                new_dist = Categorical(logits=new_logits)\n",
    "                new_log_probs = new_dist.log_prob(mb_actions)\n",
    "                entropy = new_dist.entropy().mean()\n",
    "                \n",
    "                ratio = (new_log_probs - mb_old_log_probs).exp()\n",
    "                surr1 = ratio * mb_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * mb_advantages\n",
    "                \n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = 0.5 * ((mb_returns - new_values.reshape(-1)) ** 2).mean()\n",
    "                loss = actor_loss + critic_loss - entropy_coef * entropy\n",
    "                \n",
    "                # Accumulate loss stats\n",
    "                actor_loss_sum += actor_loss.item()\n",
    "                critic_loss_sum += critic_loss.item()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "        if (ep + 1) % 500 == 0:\n",
    "            avg_rew = np.mean(rewards_history[-500:])\n",
    "            succ_rate = np.mean(success_history[-500:])\n",
    "            \n",
    "            # Normalize stats by number of updates/steps\n",
    "            # (Note: loss sums are accumulated over many mini-batches, so we divide to get avg)\n",
    "            norm_factor = max(1, updates_count * update_epochs / batch_size) \n",
    "            avg_actor_loss = actor_loss_sum / norm_factor\n",
    "            avg_critic_loss = critic_loss_sum / norm_factor\n",
    "            \n",
    "            avg_val_abs = val_abs_sum / max(1, updates_count)\n",
    "            \n",
    "            print(\n",
    "                f\"Episode {ep+1}/{episodes} | \"\n",
    "                f\"avg_reward(last500)={avg_rew:.3f} | \"\n",
    "                f\"success_rate(last500)={succ_rate:.2%} | \"\n",
    "                f\"ActorLoss={avg_actor_loss:.4f} | \"\n",
    "                f\"CriticLoss={avg_critic_loss:.4f} | \"\n",
    "                f\"Val(avg/max)={avg_val_abs:.3f}/{val_max:.3f}\"\n",
    "            )\n",
    "            \n",
    "            # Reset trackers for next 500 episodes\n",
    "            actor_loss_sum = 0.0\n",
    "            critic_loss_sum = 0.0\n",
    "            val_abs_sum = 0.0\n",
    "            val_max = 0.0\n",
    "            updates_count = 0\n",
    "\n",
    "    return rewards_history, success_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475d2db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Phase 1: train on MiniGrid-DoorKey-6x6-v0 ===\n",
      "Episode 500/3000 | avg_reward(last500)=-0.165 | success_rate(last500)=20.60% | ActorLoss=-0.0063 | CriticLoss=0.0013 | Val(avg/max)=0.095/1.142\n",
      "Episode 1000/3000 | avg_reward(last500)=-0.043 | success_rate(last500)=38.20% | ActorLoss=-0.0069 | CriticLoss=0.0019 | Val(avg/max)=0.112/0.966\n",
      "Episode 1500/3000 | avg_reward(last500)=0.529 | success_rate(last500)=88.20% | ActorLoss=-0.0092 | CriticLoss=0.0068 | Val(avg/max)=0.354/1.079\n",
      "Episode 2000/3000 | avg_reward(last500)=0.654 | success_rate(last500)=94.60% | ActorLoss=-0.0063 | CriticLoss=0.0090 | Val(avg/max)=0.460/0.983\n",
      "Episode 2500/3000 | avg_reward(last500)=0.690 | success_rate(last500)=96.40% | ActorLoss=-0.0051 | CriticLoss=0.0091 | Val(avg/max)=0.470/1.041\n",
      "Episode 3000/3000 | avg_reward(last500)=0.708 | success_rate(last500)=96.80% | ActorLoss=-0.0079 | CriticLoss=0.0101 | Val(avg/max)=0.483/1.060\n",
      "\n",
      "=== Phase 2: continue on MiniGrid-DoorKey-6x6-Enemy-v0 ===\n",
      "Episode 500/8000 | avg_reward(last500)=0.003 | success_rate(last500)=25.00% | ActorLoss=0.0049 | CriticLoss=0.0256 | Val(avg/max)=0.111/0.886\n",
      "Episode 1000/8000 | avg_reward(last500)=0.001 | success_rate(last500)=26.00% | ActorLoss=-0.0030 | CriticLoss=0.0170 | Val(avg/max)=0.086/0.465\n",
      "Episode 1500/8000 | avg_reward(last500)=0.012 | success_rate(last500)=27.80% | ActorLoss=0.0012 | CriticLoss=0.0168 | Val(avg/max)=0.096/0.621\n",
      "Episode 2000/8000 | avg_reward(last500)=-0.012 | success_rate(last500)=25.20% | ActorLoss=-0.0023 | CriticLoss=0.0152 | Val(avg/max)=0.089/0.648\n",
      "Episode 2500/8000 | avg_reward(last500)=-0.057 | success_rate(last500)=21.40% | ActorLoss=-0.0021 | CriticLoss=0.0097 | Val(avg/max)=0.072/0.639\n",
      "Episode 3000/8000 | avg_reward(last500)=-0.014 | success_rate(last500)=24.00% | ActorLoss=0.0003 | CriticLoss=0.0135 | Val(avg/max)=0.079/0.613\n",
      "Episode 3500/8000 | avg_reward(last500)=-0.078 | success_rate(last500)=19.00% | ActorLoss=-0.0035 | CriticLoss=0.0089 | Val(avg/max)=0.077/0.634\n",
      "Episode 4000/8000 | avg_reward(last500)=-0.046 | success_rate(last500)=21.20% | ActorLoss=0.0018 | CriticLoss=0.0129 | Val(avg/max)=0.078/0.573\n",
      "Episode 4500/8000 | avg_reward(last500)=-0.091 | success_rate(last500)=18.80% | ActorLoss=-0.0026 | CriticLoss=0.0071 | Val(avg/max)=0.075/0.774\n",
      "Episode 5000/8000 | avg_reward(last500)=-0.142 | success_rate(last500)=14.80% | ActorLoss=-0.0021 | CriticLoss=0.0053 | Val(avg/max)=0.069/0.682\n",
      "Episode 5500/8000 | avg_reward(last500)=-0.102 | success_rate(last500)=18.60% | ActorLoss=-0.0036 | CriticLoss=0.0071 | Val(avg/max)=0.073/0.539\n",
      "Episode 6000/8000 | avg_reward(last500)=-0.071 | success_rate(last500)=20.20% | ActorLoss=-0.0030 | CriticLoss=0.0099 | Val(avg/max)=0.078/0.648\n",
      "Episode 6500/8000 | avg_reward(last500)=-0.044 | success_rate(last500)=22.40% | ActorLoss=-0.0027 | CriticLoss=0.0117 | Val(avg/max)=0.080/0.605\n",
      "Episode 7000/8000 | avg_reward(last500)=-0.020 | success_rate(last500)=24.40% | ActorLoss=-0.0021 | CriticLoss=0.0126 | Val(avg/max)=0.074/0.710\n",
      "Episode 7500/8000 | avg_reward(last500)=-0.074 | success_rate(last500)=18.20% | ActorLoss=-0.0027 | CriticLoss=0.0108 | Val(avg/max)=0.069/0.559\n",
      "Episode 8000/8000 | avg_reward(last500)=-0.091 | success_rate(last500)=18.00% | ActorLoss=0.0012 | CriticLoss=0.0093 | Val(avg/max)=0.082/0.621\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "env_easy = gym.make(\"MiniGrid-DoorKey-6x6-v0\")\n",
    "env_hard = gym.make(\"MiniGrid-DoorKey-6x6-Enemy-v0\")\n",
    "state_dim = 5\n",
    "n_actions = len(USEFUL_ACTIONS)\n",
    "\n",
    "model = ActorCritic(state_dim, n_actions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, eps=1e-5)\n",
    "\n",
    "print(\"\\n=== Phase 1: train on MiniGrid-DoorKey-6x6-v0 ===\")\n",
    "rewards1, success1 = train_ppo(\n",
    "    env_easy, \n",
    "    model, \n",
    "    optimizer, \n",
    "    episodes=3000, \n",
    "    dist_coef=0.01,       # LOWER: Less hand-holding\n",
    "    entropy_coef=0.005,   # LOWER: Exploit faster\n",
    "    eps_clip=0.3,         # HIGHER: Allow bigger updates\n",
    "    update_epochs=5       # HIGHER: Train more on each batch\n",
    ")\n",
    "\n",
    "print(\"\\n=== Phase 2: continue on MiniGrid-DoorKey-6x6-Enemy-v0 ===\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.5e-4, eps=1e-5)\n",
    "\n",
    "rewards2, success2 = train_ppo(\n",
    "    env_hard, \n",
    "    model, \n",
    "    optimizer, \n",
    "    episodes=8000, \n",
    "    dist_coef=0.01,       # Keep sparse shaping\n",
    "    entropy_coef=0.02,    # Standard entropy for hard phase\n",
    "    update_epochs=4,      # Standard epochs\n",
    "    eps_clip=0.2          # Standard clip\n",
    ")\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
