{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b014208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from gymnasium.envs.registration import register\n",
    "import minigrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4effa8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 1) Useful actions only\n",
    "# =========================\n",
    "USEFUL_ACTIONS = [0, 1, 2, 3, 5]  # left, right, forward, pickup, toggle\n",
    "\n",
    "def sample_useful_action():\n",
    "    return int(np.random.choice(USEFUL_ACTIONS))\n",
    "\n",
    "# =========================\n",
    "# 2) State encoder\n",
    "# =========================\n",
    "def get_door_open(u):\n",
    "    for j in range(u.height):\n",
    "        for i in range(u.width):\n",
    "            obj = u.grid.get(i, j)\n",
    "            if obj is not None and obj.type == \"door\":\n",
    "                return 1 if obj.is_open else 0\n",
    "    return 0\n",
    "\n",
    "def get_state(env):\n",
    "    u = env.unwrapped\n",
    "    ax, ay = u.agent_pos\n",
    "    ad = int(u.agent_dir)\n",
    "    has_key = 1 if (u.carrying is not None and getattr(u.carrying, \"type\", None) == \"key\") else 0\n",
    "    door_open = get_door_open(u)\n",
    "    return (ax, ay, ad, has_key, door_open)\n",
    "\n",
    "# =========================\n",
    "# 3) Goal distance shaping\n",
    "# =========================\n",
    "def find_goal_pos(u):\n",
    "    for j in range(u.height):\n",
    "        for i in range(u.width):\n",
    "            obj = u.grid.get(i, j)\n",
    "            if obj is not None and obj.type == \"goal\":\n",
    "                return (i, j)\n",
    "    return None\n",
    "\n",
    "def manhattan(a, b):\n",
    "    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "# =========================\n",
    "# 4) Dyna-Q\n",
    "# =========================\n",
    "def make_Q(n_actions):\n",
    "    return defaultdict(lambda: np.zeros(n_actions, dtype=np.float32))\n",
    "\n",
    "def epsilon_greedy(Q, s, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        return sample_useful_action()\n",
    "    q = Q[s]\n",
    "    return int(max(USEFUL_ACTIONS, key=lambda a: q[a]))\n",
    "\n",
    "def train_dyna_q(\n",
    "    env,\n",
    "    Q,\n",
    "    episodes=6000,\n",
    "    max_steps=500,\n",
    "    alpha=0.15,\n",
    "    gamma=0.99,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.15,\n",
    "    eps_decay=0.99985,\n",
    "    living_penalty=-0.001,\n",
    "    dist_coef=0.02,\n",
    "    k=10,                    # planning updates per real step (after plan_start_episode)\n",
    "    plan_start_episode=1000, # DELAYED PLANNING FIX ✅\n",
    "    reward_clip_min=None     # e.g. -0.2 for enemy env; None for no clip\n",
    "):\n",
    "    \"\"\"\n",
    "    Dyna-Q:\n",
    "      - Direct RL update from real step\n",
    "      - Learn a tabular model Model[(s,a)] = (s2, r, done)\n",
    "      - After plan_start_episode, do k planning updates per real step\n",
    "    \"\"\"\n",
    "    Model = {}       # (s,a) -> (s2, r, done)\n",
    "    seen_sa = []     # list of (s,a) keys for sampling\n",
    "\n",
    "    rewards, success = [], []\n",
    "    eps = eps_start\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        s = get_state(env)\n",
    "\n",
    "        goal = find_goal_pos(env.unwrapped)\n",
    "        if goal is None:\n",
    "            raise RuntimeError(\"Goal not found in grid.\")\n",
    "\n",
    "        total_shaped = 0.0\n",
    "        last_env_r = 0.0\n",
    "\n",
    "        # --- NEW: minimal stability trackers (same as Q-learning) ---\n",
    "        td_abs_sum = 0.0\n",
    "        td_max = 0.0\n",
    "        q_abs_sum = 0.0\n",
    "        q_max = 0.0\n",
    "        updates = 0\n",
    "        # -----------------------------------------------------------\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            prev_dist = manhattan(env.unwrapped.agent_pos, goal)\n",
    "\n",
    "            a = epsilon_greedy(Q, s, eps)\n",
    "            obs2, r_env, terminated, truncated, info = env.step(a)\n",
    "            s2 = get_state(env)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            new_dist = manhattan(env.unwrapped.agent_pos, goal)\n",
    "\n",
    "            # ----- shaping aligned with reaching goal -----\n",
    "            r = float(r_env) + dist_coef * (prev_dist - new_dist) + living_penalty\n",
    "\n",
    "            # optional: stabilize frequent enemy deaths\n",
    "            if reward_clip_min is not None:\n",
    "                r = max(r, float(reward_clip_min))\n",
    "\n",
    "            # ----- (1) Direct Q update -----\n",
    "            next_best = 0.0 if done else float(max(Q[s2][aa] for aa in USEFUL_ACTIONS))\n",
    "            td_target = r + gamma * next_best\n",
    "\n",
    "            td_err = td_target - Q[s][a]\n",
    "            Q[s][a] += alpha * td_err\n",
    "\n",
    "            # --- NEW: collect diagnostics for real update ---\n",
    "            td_abs = abs(td_err)\n",
    "            td_abs_sum += td_abs\n",
    "            td_max = max(td_max, td_abs)\n",
    "\n",
    "            q_vals = Q[s]\n",
    "            q_abs_sum += float(np.mean(np.abs(q_vals)))\n",
    "            q_max = max(q_max, float(np.max(q_vals)))\n",
    "            updates += 1\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            # ----- (2) Model learning -----\n",
    "            key = (s, a)\n",
    "            if key not in Model:\n",
    "                seen_sa.append(key)\n",
    "            Model[key] = (s2, r, done)\n",
    "\n",
    "            # ----- (3) Planning (DELAYED) ✅ -----\n",
    "            effective_k = 0 if ep < plan_start_episode else k\n",
    "            if effective_k > 0 and seen_sa:\n",
    "                for _ in range(effective_k):\n",
    "                    s_p, a_p = seen_sa[np.random.randint(len(seen_sa))]\n",
    "                    s2_p, r_p, done_p = Model[(s_p, a_p)]\n",
    "                    next_best_p = 0.0 if done_p else float(max(Q[s2_p][aa] for aa in USEFUL_ACTIONS))\n",
    "                    td_target_p = r_p + gamma * next_best_p\n",
    "\n",
    "                    td_err_p = td_target_p - Q[s_p][a_p]\n",
    "                    Q[s_p][a_p] += alpha * td_err_p\n",
    "\n",
    "                    # --- NEW: diagnostics for planning updates too ---\n",
    "                    td_abs_p = abs(td_err_p)\n",
    "                    td_abs_sum += td_abs_p\n",
    "                    td_max = max(td_max, td_abs_p)\n",
    "\n",
    "                    q_vals_p = Q[s_p]\n",
    "                    q_abs_sum += float(np.mean(np.abs(q_vals_p)))\n",
    "                    q_max = max(q_max, float(np.max(q_vals_p)))\n",
    "                    updates += 1\n",
    "                    # -----------------------------------------------\n",
    "\n",
    "            total_shaped += r\n",
    "            last_env_r = float(r_env)\n",
    "            s = s2\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        eps = max(eps_end, eps * eps_decay)\n",
    "        rewards.append(total_shaped)\n",
    "        success.append(1 if last_env_r > 0 else 0)\n",
    "\n",
    "        if (ep + 1) % 500 == 0:\n",
    "            print(\n",
    "                f\"Episode {ep+1}/{episodes} | eps={eps:.3f} | \"\n",
    "                f\"avg_reward(last500)={np.mean(rewards[-500:]):.3f} | \"\n",
    "                f\"success_rate(last500)={np.mean(success[-500:]):.2%} | \"\n",
    "                f\"TDabs(avg/max)={td_abs_sum/max(1,updates):.4f}/{td_max:.4f} | \"\n",
    "                f\"Q(abs/max)={q_abs_sum/max(1,updates):.3f}/{q_max:.3f}\"\n",
    "            )\n",
    "\n",
    "    return rewards, success, eps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5dee68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Phase 1 (Dyna-Q, FAST): MiniGrid-DoorKey-6x6-v0 ===\n",
      "Episode 500/6000 | eps=0.905 | avg_reward(last500)=-0.417 | success_rate(last500)=35.20% | TDabs(avg/max)=0.0475/0.2724 | Q(abs/max)=0.253/0.575\n",
      "Episode 1000/6000 | eps=0.819 | avg_reward(last500)=-0.242 | success_rate(last500)=51.00% | TDabs(avg/max)=0.0040/0.2136 | Q(abs/max)=0.585/0.885\n",
      "Episode 1500/6000 | eps=0.741 | avg_reward(last500)=-0.302 | success_rate(last500)=46.00% | TDabs(avg/max)=0.0199/0.7578 | Q(abs/max)=0.496/0.946\n",
      "Episode 2000/6000 | eps=0.670 | avg_reward(last500)=-0.235 | success_rate(last500)=49.60% | TDabs(avg/max)=0.0043/0.4719 | Q(abs/max)=0.500/0.782\n",
      "Episode 2500/6000 | eps=0.607 | avg_reward(last500)=-0.314 | success_rate(last500)=41.20% | TDabs(avg/max)=0.0049/0.7546 | Q(abs/max)=0.555/0.844\n",
      "Episode 3000/6000 | eps=0.549 | avg_reward(last500)=-0.348 | success_rate(last500)=38.20% | TDabs(avg/max)=0.0046/0.7577 | Q(abs/max)=0.518/0.786\n",
      "Episode 3500/6000 | eps=0.497 | avg_reward(last500)=-0.352 | success_rate(last500)=38.60% | TDabs(avg/max)=0.0050/0.7516 | Q(abs/max)=0.501/0.778\n",
      "Episode 4000/6000 | eps=0.449 | avg_reward(last500)=-0.406 | success_rate(last500)=31.60% | TDabs(avg/max)=0.0037/0.8057 | Q(abs/max)=0.512/0.814\n",
      "Episode 4500/6000 | eps=0.407 | avg_reward(last500)=-0.452 | success_rate(last500)=28.80% | TDabs(avg/max)=0.0114/0.8334 | Q(abs/max)=0.529/0.892\n",
      "Episode 5000/6000 | eps=0.368 | avg_reward(last500)=-0.465 | success_rate(last500)=28.60% | TDabs(avg/max)=0.0090/0.8627 | Q(abs/max)=0.568/0.921\n",
      "Episode 5500/6000 | eps=0.333 | avg_reward(last500)=-0.523 | success_rate(last500)=21.80% | TDabs(avg/max)=0.0041/0.7127 | Q(abs/max)=0.472/0.755\n",
      "Episode 6000/6000 | eps=0.301 | avg_reward(last500)=-0.512 | success_rate(last500)=21.40% | TDabs(avg/max)=0.0048/0.6542 | Q(abs/max)=0.453/0.697\n"
     ]
    }
   ],
   "source": [
    "env_easy = gym.make(\"MiniGrid-DoorKey-6x6-v0\")\n",
    "Q = make_Q(env_easy.action_space.n)\n",
    "\n",
    "print(\"\\n=== Phase 1 (Dyna-Q, FAST): MiniGrid-DoorKey-6x6-v0 ===\")\n",
    "rewards1, success1, eps_after = train_dyna_q(\n",
    "    env_easy, Q,\n",
    "    episodes=6000,\n",
    "    max_steps=500,\n",
    "    alpha=0.15,\n",
    "    gamma=0.995,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.12,\n",
    "    eps_decay=0.99980,\n",
    "    living_penalty=-0.002,\n",
    "    dist_coef=0.02,\n",
    "    k=20,\n",
    "    plan_start_episode=800,\n",
    "    reward_clip_min=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20eb03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Phase 2 (Dyna-Q, FAST): MiniGrid-DoorKey-6x6-Enemy-v0 ===\n",
      "Episode 500/8000 | eps=0.942 | avg_reward(last500)=-0.282 | success_rate(last500)=3.20% | TDabs(avg/max)=0.0036/0.9270 | Q(abs/max)=0.617/0.944\n",
      "Episode 1000/8000 | eps=0.887 | avg_reward(last500)=-0.275 | success_rate(last500)=3.20% | TDabs(avg/max)=0.0149/0.7741 | Q(abs/max)=0.430/0.736\n",
      "Episode 1500/8000 | eps=0.835 | avg_reward(last500)=-0.253 | success_rate(last500)=4.40% | TDabs(avg/max)=0.0149/1.0438 | Q(abs/max)=0.588/0.943\n",
      "Episode 2000/8000 | eps=0.787 | avg_reward(last500)=-0.258 | success_rate(last500)=3.20% | TDabs(avg/max)=0.0183/0.9599 | Q(abs/max)=0.551/0.940\n",
      "Episode 2500/8000 | eps=0.741 | avg_reward(last500)=-0.273 | success_rate(last500)=2.60% | TDabs(avg/max)=0.0231/1.0282 | Q(abs/max)=0.566/0.903\n",
      "Episode 3000/8000 | eps=0.698 | avg_reward(last500)=-0.245 | success_rate(last500)=4.80% | TDabs(avg/max)=0.0078/0.9415 | Q(abs/max)=0.537/0.838\n",
      "Episode 3500/8000 | eps=0.657 | avg_reward(last500)=-0.263 | success_rate(last500)=3.60% | TDabs(avg/max)=0.0031/0.6842 | Q(abs/max)=0.391/0.712\n",
      "Episode 4000/8000 | eps=0.619 | avg_reward(last500)=-0.253 | success_rate(last500)=3.80% | TDabs(avg/max)=0.0116/1.0205 | Q(abs/max)=0.574/0.885\n",
      "Episode 4500/8000 | eps=0.583 | avg_reward(last500)=-0.261 | success_rate(last500)=4.00% | TDabs(avg/max)=0.0076/0.8294 | Q(abs/max)=0.409/0.658\n",
      "Episode 5000/8000 | eps=0.549 | avg_reward(last500)=-0.269 | success_rate(last500)=2.80% | TDabs(avg/max)=0.0053/0.9250 | Q(abs/max)=0.492/0.777\n",
      "Episode 5500/8000 | eps=0.517 | avg_reward(last500)=-0.249 | success_rate(last500)=4.60% | TDabs(avg/max)=0.0093/1.0428 | Q(abs/max)=0.550/0.896\n",
      "Episode 6000/8000 | eps=0.487 | avg_reward(last500)=-0.261 | success_rate(last500)=2.80% | TDabs(avg/max)=0.0061/0.9417 | Q(abs/max)=0.494/0.815\n",
      "Episode 6500/8000 | eps=0.458 | avg_reward(last500)=-0.279 | success_rate(last500)=3.00% | TDabs(avg/max)=0.0043/0.6596 | Q(abs/max)=0.352/0.674\n",
      "Episode 7000/8000 | eps=0.432 | avg_reward(last500)=-0.264 | success_rate(last500)=2.40% | TDabs(avg/max)=0.0082/0.7832 | Q(abs/max)=0.461/0.811\n",
      "Episode 7500/8000 | eps=0.407 | avg_reward(last500)=-0.263 | success_rate(last500)=3.00% | TDabs(avg/max)=0.0057/0.9309 | Q(abs/max)=0.443/0.746\n",
      "Episode 8000/8000 | eps=0.383 | avg_reward(last500)=-0.263 | success_rate(last500)=4.20% | TDabs(avg/max)=0.0183/0.8283 | Q(abs/max)=0.399/0.797\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "register(\n",
    "    id=\"MiniGrid-DoorKey-6x6-Enemy-v0\",\n",
    "    entry_point=\"enemy_doorkey_env:DoorKeyWithEnemyEnv\",\n",
    "    kwargs={\"size\": 6}\n",
    ")\n",
    "\n",
    "env_hard = gym.make(\"MiniGrid-DoorKey-6x6-Enemy-v0\")\n",
    "\n",
    "print(\"\\n=== Phase 2 (Dyna-Q, FAST): MiniGrid-DoorKey-6x6-Enemy-v0 ===\")\n",
    "rewards2, success2, _ = train_dyna_q(\n",
    "    env_hard, Q,\n",
    "    episodes=8000,\n",
    "    max_steps=500,\n",
    "    alpha=0.10,\n",
    "    gamma=0.995,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.25,\n",
    "    eps_decay=0.99988,\n",
    "    living_penalty=-0.001,\n",
    "    dist_coef=0.02,\n",
    "    k=30,\n",
    "    plan_start_episode=400,\n",
    "    reward_clip_min=-0.2\n",
    ")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
