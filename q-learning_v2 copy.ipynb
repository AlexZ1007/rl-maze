{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import defaultdict, deque, namedtuple\n",
    "from gymnasium.envs.registration import register\n",
    "import minigrid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4effa8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "=== Starting Dueling DQN Training ===\n",
      "Episode 100/3000 | ε: 0.951 | Success: 14.0% | Avg Reward: 23.29\n",
      "Episode 200/3000 | ε: 0.905 | Success: 39.0% | Avg Reward: 41.72\n",
      "Episode 300/3000 | ε: 0.861 | Success: 41.0% | Avg Reward: 43.07\n",
      "Episode 400/3000 | ε: 0.819 | Success: 62.0% | Avg Reward: 56.08\n",
      "Episode 500/3000 | ε: 0.779 | Success: 72.0% | Avg Reward: 63.47\n",
      "Episode 600/3000 | ε: 0.741 | Success: 85.0% | Avg Reward: 71.66\n",
      "Episode 700/3000 | ε: 0.705 | Success: 91.0% | Avg Reward: 75.38\n",
      "Episode 800/3000 | ε: 0.670 | Success: 91.0% | Avg Reward: 75.33\n",
      "Episode 900/3000 | ε: 0.638 | Success: 87.0% | Avg Reward: 72.70\n",
      "Episode 1000/3000 | ε: 0.606 | Success: 96.0% | Avg Reward: 78.78\n",
      "Episode 1100/3000 | ε: 0.577 | Success: 99.0% | Avg Reward: 80.75\n",
      "Episode 1200/3000 | ε: 0.549 | Success: 95.0% | Avg Reward: 78.52\n",
      "Episode 1300/3000 | ε: 0.522 | Success: 97.0% | Avg Reward: 79.67\n",
      "Episode 1400/3000 | ε: 0.496 | Success: 98.0% | Avg Reward: 80.56\n",
      "Episode 1500/3000 | ε: 0.472 | Success: 96.0% | Avg Reward: 79.15\n",
      "Episode 1600/3000 | ε: 0.449 | Success: 94.0% | Avg Reward: 77.81\n",
      "Episode 1700/3000 | ε: 0.427 | Success: 75.0% | Avg Reward: 64.71\n",
      "Episode 1800/3000 | ε: 0.406 | Success: 57.0% | Avg Reward: 53.02\n",
      "Episode 1900/3000 | ε: 0.387 | Success: 52.0% | Avg Reward: 46.48\n",
      "Episode 2000/3000 | ε: 0.368 | Success: 41.0% | Avg Reward: 39.93\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 359\u001b[39m\n\u001b[32m    356\u001b[39m env = gym.make(\u001b[33m\"\u001b[39m\u001b[33mMiniGrid-DoorKey-6x6-v0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m policy_net, rewards, success = \u001b[43mtrain_dueling_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\n\u001b[32m    363\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Final Evaluation (100 episodes) ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 223\u001b[39m, in \u001b[36mtrain_dueling_dqn\u001b[39m\u001b[34m(env, num_episodes, max_steps)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memory) >= BATCH_SIZE:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 278\u001b[39m, in \u001b[36moptimize_model\u001b[39m\u001b[34m(policy_net, target_net, optimizer, memory)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Optimize\u001b[39;00m\n\u001b[32m    277\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Gradient clipping pentru stabilitate\u001b[39;00m\n\u001b[32m    281\u001b[39m torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=\u001b[32m10.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mirun\\rl-maze\\rl-env\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mirun\\rl-maze\\rl-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mirun\\rl-maze\\rl-env\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==================== CONFIGURARE ====================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==================== DUELING DQN ARCHITECTURE ====================\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layer\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),  # Stabilitate\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128)\n",
    "        )\n",
    "        \n",
    "        # Value stream - V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream - A(s,a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_layer(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantages = self.advantage_stream(features)\n",
    "        \n",
    "        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "        # Scădem media pentru identifiabilitate\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# ==================== STATE ENCODING ====================\n",
    "def get_state(env):\n",
    "    \"\"\"Extrage starea din environment cu coordonate relative\"\"\"\n",
    "    u = env.unwrapped\n",
    "    ax, ay = u.agent_pos\n",
    "    ad = int(u.agent_dir)  # 0:Right, 1:Down, 2:Left, 3:Up\n",
    "    \n",
    "    # Găsește obiectele importante\n",
    "    key_pos, door_pos, goal_pos = None, None, None\n",
    "    door_open = 0\n",
    "    \n",
    "    for i in range(u.width):\n",
    "        for j in range(u.height):\n",
    "            obj = u.grid.get(i, j)\n",
    "            if obj:\n",
    "                if obj.type == \"key\":\n",
    "                    key_pos = (i, j)\n",
    "                elif obj.type == \"door\":\n",
    "                    door_pos = (i, j)\n",
    "                    door_open = 1 if obj.is_open else 0\n",
    "                elif obj.type == \"goal\":\n",
    "                    goal_pos = (i, j)\n",
    "    \n",
    "    # Verifică dacă agentul are cheia\n",
    "    has_key = 1 if (u.carrying and u.carrying.type == \"key\") else 0\n",
    "    \n",
    "    # Determină ținta logică (curriculum implicit)\n",
    "    if not has_key and key_pos:\n",
    "        target = key_pos\n",
    "    elif has_key and not door_open and door_pos:\n",
    "        target = door_pos\n",
    "    elif door_open and goal_pos:\n",
    "        target = goal_pos\n",
    "    else:\n",
    "        target = goal_pos if goal_pos else (ax, ay)\n",
    "    \n",
    "    # Coordonate RELATIVE (normalizate)\n",
    "    dx = (target[0] - ax) / 6.0\n",
    "    dy = (target[1] - ay) / 6.0\n",
    "    \n",
    "    # State vector: [ax, ay, dir, has_key, door_open, dx, dy]\n",
    "    state = np.array([\n",
    "        ax / 6.0,\n",
    "        ay / 6.0,\n",
    "        ad / 3.0,\n",
    "        has_key,\n",
    "        door_open,\n",
    "        dx,\n",
    "        dy\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return state\n",
    "\n",
    "# ==================== REPLAY BUFFER ====================\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# ==================== HYPERPARAMETERS ====================\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.9995\n",
    "TARGET_UPDATE = 20\n",
    "LR = 0.0003\n",
    "BUFFER_SIZE = 100000\n",
    "USEFUL_ACTIONS = [0, 1, 2, 3, 5]  # left, right, forward, pickup, toggle\n",
    "\n",
    "# ==================== TRAINING FUNCTION ====================\n",
    "def train_dueling_dqn(env, num_episodes=3000, max_steps=200):\n",
    "    \"\"\"Antrenează Dueling DQN cu stabilitate îmbunătățită\"\"\"\n",
    "    \n",
    "    # Inițializare rețele\n",
    "    policy_net = DuelingDQN(7, len(USEFUL_ACTIONS)).to(device)\n",
    "    target_net = DuelingDQN(7, len(USEFUL_ACTIONS)).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "    memory = ReplayBuffer(BUFFER_SIZE)\n",
    "    \n",
    "    # Tracking\n",
    "    episode_rewards = []\n",
    "    episode_success = []\n",
    "    success_window = deque(maxlen=100)\n",
    "    reward_window = deque(maxlen=100)\n",
    "    \n",
    "    epsilon = EPS_START\n",
    "    \n",
    "    print(\"=== Starting Dueling DQN Training ===\")\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(env)\n",
    "        total_reward = 0\n",
    "        success = 0\n",
    "        \n",
    "        # Flags pentru reward one-time\n",
    "        got_key = False\n",
    "        opened_door = False\n",
    "        prev_dist = None\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action_idx = random.randint(0, len(USEFUL_ACTIONS) - 1)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                    q_values = policy_net(state_t)\n",
    "                    action_idx = q_values.argmax().item()\n",
    "            \n",
    "            action = USEFUL_ACTIONS[action_idx]\n",
    "            \n",
    "            # Execute action\n",
    "            _, env_reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = get_state(env)\n",
    "            \n",
    "            # ==================== REWARD SHAPING ====================\n",
    "            shaped_reward = 0.0\n",
    "            \n",
    "            # 1. Living penalty (descurajează stagnarea)\n",
    "            shaped_reward -= 0.02\n",
    "            \n",
    "            # 2. Distance-based reward (progres către țintă)\n",
    "            curr_dist = abs(next_state[5]) + abs(next_state[6])\n",
    "            if prev_dist is not None:\n",
    "                dist_improvement = prev_dist - curr_dist\n",
    "                shaped_reward += dist_improvement * 8.0\n",
    "            prev_dist = curr_dist\n",
    "            \n",
    "            # 3. Milestone rewards (ONE-TIME)\n",
    "            has_key_now = (next_state[3] == 1)\n",
    "            door_open_now = (next_state[4] == 1)\n",
    "            \n",
    "            if has_key_now and not got_key:\n",
    "                shaped_reward += 15.0\n",
    "                got_key = True\n",
    "            \n",
    "            if door_open_now and not opened_door:\n",
    "                shaped_reward += 15.0\n",
    "                opened_door = True\n",
    "            \n",
    "            # 4. Success reward\n",
    "            if terminated and env_reward > 0:\n",
    "                shaped_reward += 50.0\n",
    "                success = 1\n",
    "            \n",
    "            # 5. Penalty pentru timp prea lung\n",
    "            if step > 150:\n",
    "                shaped_reward -= 0.05\n",
    "            \n",
    "            # Store transition\n",
    "            memory.push(\n",
    "                torch.FloatTensor(state),\n",
    "                torch.LongTensor([[action_idx]]),\n",
    "                torch.FloatTensor(next_state),\n",
    "                torch.FloatTensor([shaped_reward]),\n",
    "                torch.FloatTensor([float(done)])\n",
    "            )\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += shaped_reward\n",
    "            \n",
    "            # Training step\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                optimize_model(policy_net, target_net, optimizer, memory)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update target network\n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_success.append(success)\n",
    "        success_window.append(success)\n",
    "        reward_window.append(total_reward)\n",
    "        \n",
    "        # Logging\n",
    "        if episode % 100 == 0:\n",
    "            avg_success = np.mean(success_window) * 100\n",
    "            avg_reward = np.mean(reward_window)\n",
    "            print(f\"Episode {episode}/{num_episodes} | \"\n",
    "                  f\"ε: {epsilon:.3f} | \"\n",
    "                  f\"Success: {avg_success:.1f}% | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    return policy_net, episode_rewards, episode_success\n",
    "\n",
    "def optimize_model(policy_net, target_net, optimizer, memory):\n",
    "    \"\"\"Optimization step cu Double DQN\"\"\"\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    state_batch = torch.stack(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "    next_state_batch = torch.stack(batch.next_state).to(device)\n",
    "    done_batch = torch.cat(batch.done).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Double DQN: use policy_net to select action, target_net to evaluate\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_state_batch).argmax(dim=1, keepdim=True)\n",
    "        next_q = target_net(next_state_batch).gather(1, next_actions).squeeze()\n",
    "        expected_q = reward_batch + (GAMMA * next_q * (1 - done_batch))\n",
    "    \n",
    "    # Huber loss (mai robust la outliers)\n",
    "    loss = F.smooth_l1_loss(current_q.squeeze(), expected_q)\n",
    "    \n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping pentru stabilitate\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=10.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "# ==================== EVALUATION ====================\n",
    "def evaluate_agent(env, policy_net, num_episodes=100):\n",
    "    \"\"\"Evaluează agentul fără epsilon-greedy\"\"\"\n",
    "    \n",
    "    policy_net.eval()\n",
    "    successes = 0\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(env)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(200):\n",
    "            with torch.no_grad():\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                action_idx = policy_net(state_t).argmax().item()\n",
    "            \n",
    "            action = USEFUL_ACTIONS[action_idx]\n",
    "            _, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            state = get_state(env)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated and reward > 0:\n",
    "                successes += 1\n",
    "                break\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    policy_net.train()\n",
    "    \n",
    "    success_rate = (successes / num_episodes) * 100\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    \n",
    "    return success_rate, avg_reward\n",
    "\n",
    "# ==================== VISUALIZATION ====================\n",
    "def plot_results(episode_rewards, episode_success):\n",
    "    \"\"\"Plotează rezultatele antrenamentului\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Success rate\n",
    "    window = 100\n",
    "    success_smooth = np.convolve(episode_success, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(success_smooth * 100)\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Success Rate (%)')\n",
    "    ax1.set_title('Success Rate (100-episode moving average)')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Rewards\n",
    "    reward_smooth = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(reward_smooth)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Average Reward')\n",
    "    ax2.set_title('Reward (100-episode moving average)')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dueling_dqn_training.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    env = gym.make(\"MiniGrid-DoorKey-6x6-v0\")\n",
    "    \n",
    "    # Train\n",
    "    policy_net, rewards, success = train_dueling_dqn(\n",
    "        env, \n",
    "        num_episodes=3000,\n",
    "        max_steps=200\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\n=== Final Evaluation (100 episodes) ===\")\n",
    "    eval_success, eval_reward = evaluate_agent(env, policy_net, num_episodes=100)\n",
    "    print(f\"Success Rate: {eval_success:.1f}%\")\n",
    "    print(f\"Average Reward: {eval_reward:.2f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(rewards, success)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(policy_net.state_dict(), 'dueling_dqn_doorkey.pth')\n",
    "    print(\"\\nModel saved as 'dueling_dqn_doorkey.pth'\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d2db1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_Q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 5) Curriculum: Phase 1 (DoorKey) -> Phase 2 (DoorKey+Enemy)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m env_easy = gym.make(\u001b[33m\"\u001b[39m\u001b[33mMiniGrid-DoorKey-6x6-v0\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# train without rendering\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m Q = \u001b[43mmake_Q\u001b[49m(env_easy.action_space.n)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Phase 1: train on MiniGrid-DoorKey-6x6-v0 ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m rewards1, success1, eps_after = train_q(env_easy, Q,\n\u001b[32m      9\u001b[39m     episodes=\u001b[32m6000\u001b[39m, max_steps=\u001b[32m500\u001b[39m,\n\u001b[32m     10\u001b[39m     alpha=\u001b[32m0.12\u001b[39m, gamma=\u001b[32m0.99\u001b[39m,\n\u001b[32m     11\u001b[39m     eps_start=\u001b[32m1.0\u001b[39m, eps_end=\u001b[32m0.15\u001b[39m, eps_decay=\u001b[32m0.99985\u001b[39m,\n\u001b[32m     12\u001b[39m     living_penalty=-\u001b[32m0.001\u001b[39m, dist_coef=\u001b[32m0.040\u001b[39m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'make_Q' is not defined"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 5) Curriculum: Phase 1 (DoorKey) -> Phase 2 (DoorKey+Enemy)\n",
    "# ==========================================================\n",
    "env_easy = gym.make(\"MiniGrid-DoorKey-6x6-v0\")  # train without rendering\n",
    "Q = make_Q(env_easy.action_space.n)\n",
    "\n",
    "print(\"\\n=== Phase 1: train on MiniGrid-DoorKey-6x6-v0 ===\")\n",
    "rewards1, success1, eps_after = train_q(env_easy, Q,\n",
    "    episodes=6000, max_steps=500,\n",
    "    alpha=0.12, gamma=0.99,\n",
    "    eps_start=1.0, eps_end=0.15, eps_decay=0.99985,\n",
    "    living_penalty=-0.001, dist_coef=0.040\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403735c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Phase 2: continue on MiniGrid-DoorKey-6x6-Enemy-v0 ===\n",
      "Episode 500/8000 | eps=0.882 | avg_reward(last500)=-0.220 | success_rate(last500)=6.60% | TDabs(avg/max)=0.0143/0.5178 | Q(abs/max)=0.346/0.595\n",
      "Episode 1000/8000 | eps=0.865 | avg_reward(last500)=-0.238 | success_rate(last500)=4.20% | TDabs(avg/max)=0.0289/0.7388 | Q(abs/max)=0.379/0.589\n",
      "Episode 1500/8000 | eps=0.848 | avg_reward(last500)=-0.202 | success_rate(last500)=7.80% | TDabs(avg/max)=0.6304/0.6304 | Q(abs/max)=0.293/0.423\n",
      "Episode 2000/8000 | eps=0.831 | avg_reward(last500)=-0.214 | success_rate(last500)=6.80% | TDabs(avg/max)=0.0271/0.5230 | Q(abs/max)=0.266/0.396\n",
      "Episode 2500/8000 | eps=0.814 | avg_reward(last500)=-0.217 | success_rate(last500)=7.40% | TDabs(avg/max)=0.0105/0.7436 | Q(abs/max)=0.361/0.638\n",
      "Episode 3000/8000 | eps=0.798 | avg_reward(last500)=-0.190 | success_rate(last500)=9.20% | TDabs(avg/max)=0.0090/0.4488 | Q(abs/max)=0.294/0.476\n",
      "Episode 3500/8000 | eps=0.782 | avg_reward(last500)=-0.189 | success_rate(last500)=8.40% | TDabs(avg/max)=0.0086/0.5006 | Q(abs/max)=0.315/0.548\n",
      "Episode 4000/8000 | eps=0.767 | avg_reward(last500)=-0.163 | success_rate(last500)=10.40% | TDabs(avg/max)=0.0296/0.6057 | Q(abs/max)=0.254/0.431\n",
      "Episode 4500/8000 | eps=0.752 | avg_reward(last500)=-0.189 | success_rate(last500)=9.00% | TDabs(avg/max)=0.5986/0.5986 | Q(abs/max)=0.265/0.385\n",
      "Episode 5000/8000 | eps=0.737 | avg_reward(last500)=-0.160 | success_rate(last500)=11.40% | TDabs(avg/max)=0.0875/0.6615 | Q(abs/max)=0.331/0.478\n",
      "Episode 5500/8000 | eps=0.722 | avg_reward(last500)=-0.194 | success_rate(last500)=8.40% | TDabs(avg/max)=0.0144/0.7857 | Q(abs/max)=0.313/0.586\n",
      "Episode 6000/8000 | eps=0.708 | avg_reward(last500)=-0.160 | success_rate(last500)=10.20% | TDabs(avg/max)=0.0095/0.6774 | Q(abs/max)=0.316/0.477\n",
      "Episode 6500/8000 | eps=0.694 | avg_reward(last500)=-0.150 | success_rate(last500)=12.40% | TDabs(avg/max)=0.0073/0.4249 | Q(abs/max)=0.307/0.515\n",
      "Episode 7000/8000 | eps=0.680 | avg_reward(last500)=-0.152 | success_rate(last500)=13.60% | TDabs(avg/max)=0.0110/0.4245 | Q(abs/max)=0.285/0.518\n",
      "Episode 7500/8000 | eps=0.667 | avg_reward(last500)=-0.169 | success_rate(last500)=10.80% | TDabs(avg/max)=0.0086/0.4591 | Q(abs/max)=0.358/0.597\n",
      "Episode 8000/8000 | eps=0.654 | avg_reward(last500)=-0.177 | success_rate(last500)=9.60% | TDabs(avg/max)=0.1141/0.5535 | Q(abs/max)=0.248/0.363\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "env_hard = gym.make(\"MiniGrid-DoorKey-6x6-Enemy-v0\")  # train without rendering\n",
    "\n",
    "print(\"\\n=== Phase 2: continue on MiniGrid-DoorKey-6x6-Enemy-v0 ===\")\n",
    "rewards2, success2, _ = train_q(env_hard, Q,\n",
    "    episodes=8000, max_steps=500,\n",
    "    alpha=0.06, gamma=0.99,\n",
    "    eps_start=0.9, eps_end=0.45, eps_decay=0.99996,\n",
    "    living_penalty=-0.001, dist_coef=0.040\n",
    ")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
