{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b014208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\unibuc mafia\\rl\\maze-rl\\rl-env\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from gymnasium.envs.registration import register\n",
    "import minigrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4effa8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 0) Register your custom env (assumes enemy_doorkey_env.py exists)\n",
    "# ==========================================================\n",
    "register(\n",
    "    id=\"MiniGrid-DoorKey-6x6-Enemy-v0\",\n",
    "    entry_point=\"enemy_doorkey_env:DoorKeyWithEnemyEnv\",\n",
    "    kwargs={\"size\":6}\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# 1) Useful actions only (DoorKey)\n",
    "# ==========================================================\n",
    "USEFUL_ACTIONS = [0, 1, 2, 3, 5]  # left, right, forward, pickup, toggle\n",
    "\n",
    "def sample_useful_action():\n",
    "    return int(np.random.choice(USEFUL_ACTIONS))\n",
    "\n",
    "# ==========================================================\n",
    "# 2) State encoder (no enemy_pos -> smaller table)\n",
    "# ==========================================================\n",
    "def get_door_open(u):\n",
    "    for j in range(u.height):\n",
    "        for i in range(u.width):\n",
    "            obj = u.grid.get(i, j)\n",
    "            if obj is not None and obj.type == \"door\":\n",
    "                return 1 if obj.is_open else 0\n",
    "    return 0\n",
    "\n",
    "def get_state(env):\n",
    "    u = env.unwrapped\n",
    "    ax, ay = u.agent_pos\n",
    "    ad = int(u.agent_dir)\n",
    "    has_key = 1 if (u.carrying is not None and getattr(u.carrying, \"type\", None) == \"key\") else 0\n",
    "    door_open = get_door_open(u)\n",
    "    return (ax, ay, ad, has_key, door_open)\n",
    "\n",
    "# ==========================================================\n",
    "# 3) Distance-to-goal shaping helpers (aligned with success)\n",
    "# ==========================================================\n",
    "def find_goal_pos(u):\n",
    "    for j in range(u.height):\n",
    "        for i in range(u.width):\n",
    "            obj = u.grid.get(i, j)\n",
    "            if obj is not None and obj.type == \"goal\":\n",
    "                return (i, j)\n",
    "    return None\n",
    "\n",
    "def manhattan(a, b):\n",
    "    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "# ==========================================================\n",
    "# 4) Q-learning\n",
    "# ==========================================================\n",
    "def make_Q(n_actions):\n",
    "    return defaultdict(lambda: np.zeros(n_actions, dtype=np.float32))\n",
    "\n",
    "def epsilon_greedy(Q, s, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        return sample_useful_action()\n",
    "    q = Q[s]\n",
    "    return int(max(USEFUL_ACTIONS, key=lambda a: q[a]))\n",
    "\n",
    "def train_q(\n",
    "    env,\n",
    "    Q,\n",
    "    episodes=6000,\n",
    "    max_steps=500,\n",
    "    alpha=0.15,\n",
    "    gamma=0.99,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.15,\n",
    "    eps_decay=0.99985,\n",
    "    living_penalty=-0.001,\n",
    "    dist_coef=0.02,\n",
    "):\n",
    "    rewards, success = [], []\n",
    "    eps = eps_start\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        s = get_state(env)\n",
    "\n",
    "        goal = find_goal_pos(env.unwrapped)\n",
    "        if goal is None:\n",
    "            raise RuntimeError(\"Goal not found in grid. Check environment generation.\")\n",
    "\n",
    "        total_shaped = 0.0\n",
    "        last_env_r = 0.0\n",
    "\n",
    "        # --- NEW: minimal stability trackers ---\n",
    "        td_abs_sum = 0.0\n",
    "        td_max = 0.0\n",
    "        q_abs_sum = 0.0\n",
    "        q_max = 0.0\n",
    "        updates = 0\n",
    "        # -------------------------------------\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            prev_dist = manhattan(env.unwrapped.agent_pos, goal)\n",
    "\n",
    "            a = epsilon_greedy(Q, s, eps)\n",
    "            obs2, r, terminated, truncated, info = env.step(a)\n",
    "\n",
    "            r = max(float(r), -0.2)   # clip negative rewards\n",
    "            last_env_r = r\n",
    "\n",
    "            s2 = get_state(env)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            new_dist = manhattan(env.unwrapped.agent_pos, goal)\n",
    "\n",
    "            # reward shaping\n",
    "            shaped = r\n",
    "            shaped += dist_coef * (prev_dist - new_dist)\n",
    "            shaped += living_penalty\n",
    "\n",
    "            # Q-learning update (max over useful actions)\n",
    "            next_best = 0.0 if done else float(max(Q[s2][aa] for aa in USEFUL_ACTIONS))\n",
    "            td_target = shaped + gamma * next_best\n",
    "            td_err = td_target - Q[s][a]\n",
    "\n",
    "            Q[s][a] += alpha * td_err\n",
    "\n",
    "            # --- NEW: collect minimal diagnostics ---\n",
    "            td_abs = abs(td_err)\n",
    "            td_abs_sum += td_abs\n",
    "            td_max = max(td_max, td_abs)\n",
    "\n",
    "            q_vals = Q[s]\n",
    "            q_abs_sum += np.mean(np.abs(q_vals))\n",
    "            q_max = max(q_max, np.max(q_vals))\n",
    "            updates += 1\n",
    "            # ---------------------------------------\n",
    "\n",
    "            total_shaped += shaped\n",
    "            s = s2\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        eps = max(eps_end, eps * eps_decay)\n",
    "        rewards.append(total_shaped)\n",
    "        success.append(1 if last_env_r > 0 else 0)\n",
    "\n",
    "        if (ep + 1) % 500 == 0:\n",
    "            print(\n",
    "                f\"Episode {ep+1}/{episodes} | eps={eps:.3f} | \"\n",
    "                f\"avg_reward(last500)={np.mean(rewards[-500:]):.3f} | \"\n",
    "                f\"success_rate(last500)={np.mean(success[-500:]):.2%} | \"\n",
    "                f\"TDabs(avg/max)={td_abs_sum/max(1,updates):.4f}/{td_max:.4f} | \"\n",
    "                f\"Q(abs/max)={q_abs_sum/max(1,updates):.3f}/{q_max:.3f}\"\n",
    "            )\n",
    "\n",
    "    return rewards, success, eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475d2db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Phase 1: train on MiniGrid-DoorKey-6x6-v0 ===\n",
      "Episode 500/6000 | eps=0.928 | avg_reward(last500)=-0.113 | success_rate(last500)=31.40% | TDabs(avg/max)=0.0204/0.2397 | Q(abs/max)=0.354/0.629\n",
      "Episode 1000/6000 | eps=0.861 | avg_reward(last500)=0.105 | success_rate(last500)=56.80% | TDabs(avg/max)=0.0146/0.4222 | Q(abs/max)=0.397/0.662\n",
      "Episode 1500/6000 | eps=0.799 | avg_reward(last500)=0.197 | success_rate(last500)=63.60% | TDabs(avg/max)=0.0115/0.5731 | Q(abs/max)=0.399/0.648\n",
      "Episode 2000/6000 | eps=0.741 | avg_reward(last500)=0.334 | success_rate(last500)=73.60% | TDabs(avg/max)=0.0051/0.5289 | Q(abs/max)=0.388/0.639\n",
      "Episode 2500/6000 | eps=0.687 | avg_reward(last500)=0.252 | success_rate(last500)=67.40% | TDabs(avg/max)=0.0242/0.3118 | Q(abs/max)=0.406/0.670\n",
      "Episode 3000/6000 | eps=0.638 | avg_reward(last500)=0.446 | success_rate(last500)=82.00% | TDabs(avg/max)=0.0118/0.1732 | Q(abs/max)=0.508/0.815\n",
      "Episode 3500/6000 | eps=0.592 | avg_reward(last500)=0.334 | success_rate(last500)=71.60% | TDabs(avg/max)=0.0292/0.2982 | Q(abs/max)=0.444/0.759\n",
      "Episode 4000/6000 | eps=0.549 | avg_reward(last500)=0.455 | success_rate(last500)=79.60% | TDabs(avg/max)=0.0127/0.2420 | Q(abs/max)=0.440/0.742\n",
      "Episode 4500/6000 | eps=0.509 | avg_reward(last500)=0.350 | success_rate(last500)=69.80% | TDabs(avg/max)=0.0110/0.1038 | Q(abs/max)=0.419/0.822\n",
      "Episode 5000/6000 | eps=0.472 | avg_reward(last500)=0.284 | success_rate(last500)=65.20% | TDabs(avg/max)=0.0156/0.1378 | Q(abs/max)=0.363/0.784\n",
      "Episode 5500/6000 | eps=0.438 | avg_reward(last500)=0.301 | success_rate(last500)=66.60% | TDabs(avg/max)=0.0091/0.5604 | Q(abs/max)=0.414/0.710\n",
      "Episode 6000/6000 | eps=0.407 | avg_reward(last500)=0.387 | success_rate(last500)=73.20% | TDabs(avg/max)=0.0129/0.1071 | Q(abs/max)=0.315/0.777\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 5) Curriculum: Phase 1 (DoorKey) -> Phase 2 (DoorKey+Enemy)\n",
    "# ==========================================================\n",
    "env_easy = gym.make(\"MiniGrid-DoorKey-6x6-v0\")  # train without rendering\n",
    "Q = make_Q(env_easy.action_space.n)\n",
    "\n",
    "print(\"\\n=== Phase 1: train on MiniGrid-DoorKey-6x6-v0 ===\")\n",
    "rewards1, success1, eps_after = train_q(env_easy, Q,\n",
    "    episodes=6000, max_steps=500,\n",
    "    alpha=0.12, gamma=0.99,\n",
    "    eps_start=1.0, eps_end=0.15, eps_decay=0.99985,\n",
    "    living_penalty=-0.001, dist_coef=0.040\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403735c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Phase 2: continue on MiniGrid-DoorKey-6x6-Enemy-v0 ===\n",
      "Episode 500/8000 | eps=0.882 | avg_reward(last500)=-0.220 | success_rate(last500)=6.60% | TDabs(avg/max)=0.0143/0.5178 | Q(abs/max)=0.346/0.595\n",
      "Episode 1000/8000 | eps=0.865 | avg_reward(last500)=-0.238 | success_rate(last500)=4.20% | TDabs(avg/max)=0.0289/0.7388 | Q(abs/max)=0.379/0.589\n",
      "Episode 1500/8000 | eps=0.848 | avg_reward(last500)=-0.202 | success_rate(last500)=7.80% | TDabs(avg/max)=0.6304/0.6304 | Q(abs/max)=0.293/0.423\n",
      "Episode 2000/8000 | eps=0.831 | avg_reward(last500)=-0.214 | success_rate(last500)=6.80% | TDabs(avg/max)=0.0271/0.5230 | Q(abs/max)=0.266/0.396\n",
      "Episode 2500/8000 | eps=0.814 | avg_reward(last500)=-0.217 | success_rate(last500)=7.40% | TDabs(avg/max)=0.0105/0.7436 | Q(abs/max)=0.361/0.638\n",
      "Episode 3000/8000 | eps=0.798 | avg_reward(last500)=-0.190 | success_rate(last500)=9.20% | TDabs(avg/max)=0.0090/0.4488 | Q(abs/max)=0.294/0.476\n",
      "Episode 3500/8000 | eps=0.782 | avg_reward(last500)=-0.189 | success_rate(last500)=8.40% | TDabs(avg/max)=0.0086/0.5006 | Q(abs/max)=0.315/0.548\n",
      "Episode 4000/8000 | eps=0.767 | avg_reward(last500)=-0.163 | success_rate(last500)=10.40% | TDabs(avg/max)=0.0296/0.6057 | Q(abs/max)=0.254/0.431\n",
      "Episode 4500/8000 | eps=0.752 | avg_reward(last500)=-0.189 | success_rate(last500)=9.00% | TDabs(avg/max)=0.5986/0.5986 | Q(abs/max)=0.265/0.385\n",
      "Episode 5000/8000 | eps=0.737 | avg_reward(last500)=-0.160 | success_rate(last500)=11.40% | TDabs(avg/max)=0.0875/0.6615 | Q(abs/max)=0.331/0.478\n",
      "Episode 5500/8000 | eps=0.722 | avg_reward(last500)=-0.194 | success_rate(last500)=8.40% | TDabs(avg/max)=0.0144/0.7857 | Q(abs/max)=0.313/0.586\n",
      "Episode 6000/8000 | eps=0.708 | avg_reward(last500)=-0.160 | success_rate(last500)=10.20% | TDabs(avg/max)=0.0095/0.6774 | Q(abs/max)=0.316/0.477\n",
      "Episode 6500/8000 | eps=0.694 | avg_reward(last500)=-0.150 | success_rate(last500)=12.40% | TDabs(avg/max)=0.0073/0.4249 | Q(abs/max)=0.307/0.515\n",
      "Episode 7000/8000 | eps=0.680 | avg_reward(last500)=-0.152 | success_rate(last500)=13.60% | TDabs(avg/max)=0.0110/0.4245 | Q(abs/max)=0.285/0.518\n",
      "Episode 7500/8000 | eps=0.667 | avg_reward(last500)=-0.169 | success_rate(last500)=10.80% | TDabs(avg/max)=0.0086/0.4591 | Q(abs/max)=0.358/0.597\n",
      "Episode 8000/8000 | eps=0.654 | avg_reward(last500)=-0.177 | success_rate(last500)=9.60% | TDabs(avg/max)=0.1141/0.5535 | Q(abs/max)=0.248/0.363\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "env_hard = gym.make(\"MiniGrid-DoorKey-6x6-Enemy-v0\")  # train without rendering\n",
    "\n",
    "print(\"\\n=== Phase 2: continue on MiniGrid-DoorKey-6x6-Enemy-v0 ===\")\n",
    "rewards2, success2, _ = train_q(env_hard, Q,\n",
    "    episodes=8000, max_steps=500,\n",
    "    alpha=0.06, gamma=0.99,\n",
    "    eps_start=0.9, eps_end=0.45, eps_decay=0.99996,\n",
    "    living_penalty=-0.001, dist_coef=0.040\n",
    ")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
